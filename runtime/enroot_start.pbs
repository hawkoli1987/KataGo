#!/bin/bash
#PBS -N go_engine
#PBS -q AISG_debug
#PBS -l walltime=24:00:00
#PBS -l select=1:mem=200gb:ncpus=32:ngpus=1:host=hopper-34
#PBS -j oe
#PBS -W umask=0007
#PBS -o /scratch_aisg/SPEC-SF-AISG/yuli/ARF-Training/repos/KataGo/runtime/log/

set -euo pipefail

# Redirect stdout/stderr to log files
LOG_DIR="/scratch_aisg/SPEC-SF-AISG/yuli/ARF-Training/repos/KataGo/runtime/log"
mkdir -p "${LOG_DIR}"
exec >> "${LOG_DIR}/out.log" 2>> "${LOG_DIR}/err.log"

# Configuration
export SHARED_FS="/scratch/Projects/SPEC-SF-AISG"
export SQSH_DIR="${SHARED_FS}/sqsh"
export SQSH_FILE="${SQSH_DIR}/go.sqsh"
export CONTAINER_NAME="katago_dev"
export ENROOT_DATA_PATH="${SHARED_FS}/.enroot/data"
export HOST_NVIDIA_MOUNT="/opt/host-nvidia"

# Server port (default 9100, matching config.yaml)
PORT="${ANALYSIS_PORT:-9100}"

mkdir -p "${ENROOT_DATA_PATH}"

if [ ! -f "${SQSH_FILE}" ]; then
  echo "Missing ${SQSH_FILE}. Run runtime/enroot_create.pbs first." >&2
  exit 1
fi

# Find CUDA libraries dynamically (different paths on different nodes)
find_cuda_lib() {
    local lib_name="$1"
    local search_paths=(
        "/lib64/${lib_name}"
        "/usr/lib64/${lib_name}"
        "/usr/lib/x86_64-linux-gnu/${lib_name}"
        "/opt/nvidia/lib64/${lib_name}"
    )
    for path in "${search_paths[@]}"; do
        if [ -f "$path" ]; then
            echo "$path"
            return 0
        fi
    done
    # Try ldconfig as fallback
    ldconfig -p 2>/dev/null | grep -m1 "${lib_name}" | awk '{print $NF}' | head -1
}

HOST_LIBCUDA=$(find_cuda_lib "libcuda.so.1")
HOST_LIBNVML=$(find_cuda_lib "libnvidia-ml.so.1")

echo "Found libcuda: ${HOST_LIBCUDA:-NOT FOUND}"
echo "Found libnvml: ${HOST_LIBNVML:-NOT FOUND}"

if [ -z "${HOST_LIBCUDA}" ]; then
    echo "ERROR: Could not find libcuda.so.1 on this node" >&2
    echo "Searched: /lib64, /usr/lib64, /usr/lib/x86_64-linux-gnu, /opt/nvidia/lib64" >&2
    exit 1
fi

export LD_LIBRARY_PATH="${HOST_NVIDIA_MOUNT}:${LD_LIBRARY_PATH:-}"

# Create container if it doesn't exist
if ! enroot list | grep -q "^${CONTAINER_NAME}$"; then
    enroot create -n "${CONTAINER_NAME}" "${SQSH_FILE}"
fi

# Ensure mount target exists inside the container rootfs
enroot start --root --rw \
    --mount="${HOME}:${HOME}" \
    --mount="${SHARED_FS}:${SHARED_FS}" \
  "${CONTAINER_NAME}" \
  /bin/bash -lc "mkdir -p ${HOST_NVIDIA_MOUNT}"

echo "Starting analysis server on port ${PORT}..."

# Build mount options dynamically
MOUNT_OPTS=(
    "--mount=${HOME}:${HOME}"
    "--mount=${SHARED_FS}:${SHARED_FS}"
    "--mount=${HOST_LIBCUDA}:${HOST_NVIDIA_MOUNT}/libcuda.so.1"
)

# Add optional mounts if files exist
[ -n "${HOST_LIBNVML}" ] && [ -f "${HOST_LIBNVML}" ] && \
    MOUNT_OPTS+=("--mount=${HOST_LIBNVML}:${HOST_NVIDIA_MOUNT}/libnvidia-ml.so.1")
[ -f "/usr/bin/nvidia-smi" ] && \
    MOUNT_OPTS+=("--mount=/usr/bin/nvidia-smi:/usr/bin/nvidia-smi")

# Start the analysis server
enroot start --root --rw \
    --env LD_LIBRARY_PATH \
    "${MOUNT_OPTS[@]}" \
  "${CONTAINER_NAME}" \
  /bin/bash -lc "cd /scratch/Projects/SPEC-SF-AISG/source_files/KataGo && uvicorn runtime.analysis_server:app --host 0.0.0.0 --port ${PORT}"
